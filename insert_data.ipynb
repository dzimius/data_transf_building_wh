{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook prepares froam a csv file (sample kaggle data: https://www.kaggle.com/datasets/ishanshrivastava28/tata-online-retail-dataset) and inserts it into a SQL table \n",
    "# It serves as a starting point for a simulated, quasi-real business case focused on building a simple database (Data Warehouse, Star Schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('data/Online_Retail_Data_Set.csv')\n",
    "\n",
    "# Step 2: Add random Client and Email columns\n",
    "\n",
    "unique_ids = df['CustomerID'].unique()\n",
    "n_unique = len(unique_ids)\n",
    "\n",
    "# Pre-generate a pool of names\n",
    "pool_size = n_unique\n",
    "first_names_pool = [names.get_first_name() for _ in range(n_unique)]\n",
    "last_names_pool = [names.get_last_name() for _ in range(n_unique)]\n",
    "\n",
    "# Use np.random.choice to vectorize the selection for each row\n",
    "first_names_unique = np.random.choice(first_names_pool, size=n_unique)\n",
    "last_names_unique = np.random.choice(last_names_pool, size=n_unique)\n",
    "\n",
    "clients_unique = pd.Series(first_names_unique) + ' ' + pd.Series(last_names_unique)\n",
    "emails_unique = pd.Series(first_names_unique).str.lower() + '.' + pd.Series(last_names_unique).str.lower() + '@example.com'\n",
    "\n",
    "# Build mapping dictionaries from CustomerID to client name and email\n",
    "client_map = dict(zip(unique_ids, clients_unique))\n",
    "email_map = dict(zip(unique_ids, emails_unique))\n",
    "\n",
    "\n",
    "# Map these values back to the original DataFrame so that each CustomerID gets consistent values\n",
    "df['Client'] = df['CustomerID'].map(client_map)\n",
    "df['Email'] = df['CustomerID'].map(email_map)\n",
    "\n",
    "# Step 3: Clean the DataFrame\n",
    "# Convert InvoiceDate column to datetime; invalid parsing will result in NaT\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "\n",
    "# Ensure numeric columns are properly converted\n",
    "df['UnitPrice'] = pd.to_numeric(df['UnitPrice'], errors='coerce')\n",
    "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce', downcast='integer')\n",
    "\n",
    "# Convert CustomerID to string (NVARCHAR expected); this will turn missing values into 'nan'\n",
    "df['CustomerID'] = df['CustomerID'].apply(lambda x: str(int(float(x))) if pd.notnull(x) else None)\n",
    "# Replace pandas NaN with Python None so that pyodbc will insert SQL NULLs\n",
    "df = df.where(pd.notnull(df), None)\n",
    "\n",
    "# Select the columns in the order of the table definition\n",
    "columns = ['InvoiceNo', 'StockCode', 'DescriptionProd', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country', 'Client', 'Email']\n",
    "\n",
    "# Convert the DataFrame rows to a list of tuples\n",
    "data_to_insert = list(df[columns].itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create table schema in SQL Server\n",
    "\n",
    "\n",
    "# Define connection string to SQL Server\n",
    "conn_str = (\n",
    "    \"DRIVER={SQL Server};\"\n",
    "    \"SERVER=maciek_d;\"\n",
    "    \"DATABASE=retail_online;\"\n",
    "    \"Trusted_Connection=yes;\"\n",
    ")\n",
    "\n",
    "# Establish the connection\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Drop the table if it already exists (this removes the old schema and data)\n",
    "drop_table_sql = \"IF OBJECT_ID('dbo.OrginalData', 'U') IS NOT NULL DROP TABLE dbo.OrginalData;\"\n",
    "cursor.execute(drop_table_sql)\n",
    "conn.commit()\n",
    "\n",
    "# Create the new table with the desired schema\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE OrginalData (\n",
    "    InvoiceNo NVARCHAR(50),\n",
    "    StockCode NVARCHAR(50),\n",
    "    DescriptionProd NVARCHAR(255),\n",
    "    Quantity INT,\n",
    "    InvoiceDate DATETIME,\n",
    "    UnitPrice DECIMAL(10,2),\n",
    "    CustomerID NVARCHAR(50),\n",
    "    Country NVARCHAR(100),\n",
    "    Client NVARCHAR(50),\n",
    "    Email NVARCHAR(50)\n",
    "\n",
    "\n",
    ");\n",
    "\"\"\"\n",
    "cursor.execute(create_table_sql)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Insert Data Directly from DataFrame to SQL Server ---\n",
    "\n",
    "# Prepare the INSERT statement with placeholders\n",
    "insert_sql = \"\"\"\n",
    "INSERT INTO OrginalData (InvoiceNo, StockCode, DescriptionProd, Quantity, InvoiceDate, UnitPrice, CustomerID, Country, Client, Email)\n",
    "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "\"\"\"\n",
    "\n",
    "# Enable fast executemany for better performance on large datasets\n",
    "cursor.fast_executemany = True\n",
    "\n",
    "# Execute the bulk insert\n",
    "cursor.executemany(insert_sql, data_to_insert)\n",
    "conn.commit()\n",
    "\n",
    "# --- Cleanup: Close the Connection ---\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
